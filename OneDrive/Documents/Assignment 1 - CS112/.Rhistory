ggtitle("Delay time across Circulation Date")+
xlab("Circulation date")+
ylab("Days")
###Graph to see if the duration actually decreases.
library(ggplot2)
ggplot(data = foo2sorted, aes(x=foo2sorted$CirculationDate, y=as.numeric(foo2sorted$RevisedCompletionDate - foo2sorted$OriginalCompletionDate)))+
geom_line(color="#E69F01")+
geom_point()+
ggtitle("Delay time across Circulation Date")+
xlab("Circulation date")+
ylab("Days")
regression_1 <- lm((as.numeric(delay_sorted) ~ as.numeric(time_sorted)), data = foo2sorted)
foo3 %>% foo3[filter(!=na)] #eliminate NAs
foo3 %>% foo3[filter(!is.na(foo3$Rating))] #eliminate NAs
foo3 %>% filter(!is.na(foo3$Rating)) #eliminate NAs
foo3 <- foo2sorted[which(x >= "2010-01-01"),]
foo3 %>% filter(!is.na(foo3$Rating)) #eliminate NAs
prop.table(table(foo3$Rating))
foo2Rev <- foo2[order(foo2$RevisedAmount),]
print(foo2Rev)
First10 <- foo2Rev[2:166,] #because the first 1 was NA
Last10 <- foo2Rev[1486:1650,]
lm1 <- lm(Jackiedf$height ~ Jackiedf$exercise_time + Jackiedf$milk_ml)
library(Matching)
data(Jackiedf)
lm1 <- lm(Jackiedf$height ~ Jackiedf$exercise_time + Jackiedf$milk_ml)
lm1$coef
library(arm)
set.seed(123)
sim_results <- sim(lm1, n.sims = 20)
library(arm)
library(Matching)
data(Jackiedf)
Jackiedf = read.csv("CS112.2.1.preclass_ss1.csv")
lm1 <- lm(Jackiedf$height ~ Jackiedf$exercise_time + Jackiedf$milk_ml)
lm1$coef
library(arm)
set.seed(123)
sim_results <- sim(lm1, n.sims = 20)
exercise_time <- 10:209
exercise_time
milk_ml <- runif(200)
milk_ml # to see the variable you created
height <- 0.2*exercise_time + 1.4*milk_ml + 100 + rnorm(200)
cor(milk_ml,height)
height
cor(exercise_time,height,use ='complete.obs')
barplot(exercise_time, main = "Exercise time", xlab = "minutes/week")
tall <- cbind(exercise_time, milk_ml,parents_height,height)
barplot(milk_ml, main = "Milk consumption per week", xlab = "liter/week")
barplot(parents_height, main = "Parents heights", xlab = "cm")
barplot(height, main = "Observed heights", xlab = "cm")
##To dataframe
df <- data.frame(exercise_time,milk_ml, parents_height,height)
dim(df)
head(df)
##Export to CSV file
picked = sample(seq_len(nrow(df)),size = 100)
df_ss1 =df[picked,]
df_ss2 =df[-picked,]
head(df_ss1)
dim(df_ss1)
head(df_ss2)
dim(df_ss2)
write.csv(df_ss1, file = 'noise CS112.2.1.preclass_ss1.csv', row.names = TRUE)
write.csv(df_ss2, file = 'noise CS112.2.1.preclass_ss2.csv', row.names = TRUE)
barplot(exercise_time,height)
library(arm)
library(Matching)
data(Jackiedf)
Jackiedf = read.csv("noise CS112.2.1.preclass_ss1.csv")
df = read.csv("noise CS112.2.1.preclass_ss1.csv")
lm1 <- lm(Jackiedf$height ~ Jackiedf$exercise_time + Jackiedf$milk_ml)
lm1$coef
library(arm)
library(arm)
library(Matching)
install.packages("arm")
library(arm)
library(Matching)
df = read.csv("noise CS112.2.1.preclass_ss1.csv")
lm1 <- lm(Jackiedf$height ~ Jackiedf$exercise_time + Jackiedf$milk_ml)
lm1$coef
library(arm)
install.packages("Matching")
library(arm)
library(Matching)
df = read.csv("noise CS112.2.1.preclass_ss1.csv")
lm1 <- lm(Jackiedf$height ~ Jackiedf$exercise_time + Jackiedf$milk_ml)
lm1$coef
library(arm)
set.seed(123)
sim_results <- sim(lm1, n.sims = 20)
set.seed(123)
sim_results <- sim(lm1, n.sims = 20)
set.seed(232)
# 20 sims is too few to get reliable results
sim_results2 <- sim(lm1, n.sims = 10000)
mean(sim_results2@coef[,1])
mean(sim_results2@coef[,2])
# 53 -- compare to 53.39 (ditto re large number of simulations)
# 53 -- compare to 53.39 (ditto re large number of simulations)
# 53 -- compare to 53.39 (ditto re large number of simulations)
first_set_of_simulated_ys_including_irreducible_error <-
sim_results2@coef[1,1]*rep(1, length(lalonde$age)) +
sim_results2@coef[1,2]*lalonde$age +
rnorm(length(lalonde$age), 0, sim_results2@sigma[1])
# 20 sims is too few to get reliable results
sim_results2 <- sim(lm1, n.sims = 10000)
mean(sim_results2@coef[,1])
mean(sim_results2@coef[,2])
first_set_of_simulated_ys_including_irreducible_error <-
sim_results2@coef[1,1]*rep(1, length(lalonde$age)) +
sim_results2@coef[1,2]*lalonde$age +
rnorm(length(lalonde$age), 0, sim_results2@sigma[1])
second_set_of_simulated_ys_including_irreducible_error <-
sim_results2@coef[2,1]*rep(1, length(lalonde$age)) +
sim_results2@coef[2,2]*lalonde$age +
rnorm(length(lalonde$age), 0, sim_results2@sigma[2])
# obviously, in practice, the above would be done via something like a loop
# obviously, in practice, the above would be done via something like a loop
# and you could do it for each of your simulated estimates (in this case, 10K)
# obviously, in practice, the above would be done via something like a loop
# and you could do it for each of your simulated estimates (in this case, 10K)
# and once you had these simulated ys, you could estimate prediction intervals of y
first_set_of_simulated_ys_including_irreducible_error <-
sim_results2@coef[1,2]*df$exercise_time +
rnorm(length(df$exercise_time), 0, sim_results2@sigma[1])
second_set_of_simulated_ys_including_irreducible_error <-
sim_results2@coef[2,1]*rep(1, length(lalonde$age)) +
sim_results2@coef[2,2]*lalonde$age +
rnorm(length(lalonde$age), 0, sim_results2@sigma[2])
second_set_of_simulated_ys_including_irreducible_error <-
sim_results2@coef[2,1]*rep(1, length(df$exercise_time)) +
sim_results2@coef[2,2]*df$exercise_time +
rnorm(length(df$exercise_time), 0, sim_results2@sigma[2])
install.packages("arm")
install.packages("arm")
library(arm)
sesame <- read.csv(url("https://docs.google.com/spreadsheets/d/e/2PACX-1vQC5enEj91bsrAmXER5z0eC_xlUbe_rhYiXEeTlb90-w1pxbqfmejfRwaCvRZzm201_nSlS-ZG0MN70/pub?gid=1873712132&single=true&output=csv"))
lm.4 <- lm (post.test ~ treatment + pre.test + I(treatment*pre.test), data = sesame)
install.packages("arm")
install.packages("arm")
install.packages("arm")
sesame <- read.csv(url("https://docs.google.com/spreadsheets/d/e/2PACX-1vQC5enEj91bsrAmXER5z0eC_xlUbe_rhYiXEeTlb90-w1pxbqfmejfRwaCvRZzm201_nSlS-ZG0MN70/pub?gid=1873712132&single=true&output=csv"))
lm.4 <- lm (post.test ~ treatment + pre.test + I(treatment*pre.test), data = sesame)
lm.4.equivalent <- lm(post.test ~ treatment*pre.test, data = sesame)
lm.4.equivalent2 <- lm(post.test ~ treatment + pre.test + treatment:pre.test, data = sesame)
display (lm.4)
###
lm.4.sim <- sim (lm.4, n.sims = 1000)
(lm.4.sim)@coef # explore the simulated coefficients all equally likely under this model
lm.4.sim@sigma # explore the simulated estimates of fundamental uncertainty
##
summary(lm.4)
plot (0, 0, xlim=c(80, 120), ylim=c(-5,10),
xlab="pre-test", ylab="treatment effect",
main="treatment effect in grade 4")
abline (h = 0, lwd=.5, lty=2) # draws a horizontal line
abline (a = coef(lm.4.sim)[1,2], b = coef(lm.4.sim)[1,4],
lwd = .5, col = "gray")
for (i in 1:1000) {
abline (a = coef(lm.4.sim)[i,2], b = coef(lm.4.sim)[i,4],
lwd = .5, col = "gray")
}
abline (a = mean(coef(lm.4.sim)[2]),
b = mean(coef(lm.4.sim)[2]),
lwd = 3, col = "purple4")
abline (a = mean(coef(lm.4.sim)[2]), b = mean(coef(lm.4.sim)[2]),
lwd = 3, col = "purple4")
###
abline (a = mean(coef(lm.4.sim)[???]),
b = mean(coef(lm.4.sim)[???]),
lwd = 3, col = "purple4")
abline (a = mean(coef(lm.4.sim)[???]), b = mean(coef(lm.4.sim)[???]),
lwd = 3, col = "purple4")
lm.4.equivalent <- lm(post.test ~ treatment*pre.test, data = sesame)
display(lm.4.equivalent)
display (lm.4)
###
abline (a = mean(coef(lm.4.sim)[4]),
b = mean(coef(lm.4.sim)[4]),
lwd = 3, col = "purple4")
abline (a = mean(coef(lm.4.sim)[4]), b = mean(coef(lm.4.sim)[???]),
lwd = 3, col = "purple4")
###
abline (a = mean(coef(lm.4.sim)[4]),
b = mean(coef(lm.4.sim)[4]),
lwd = 3, col = "purple4")
abline (a = mean(coef(lm.4.sim)[4]), b = mean(coef(lm.4.sim)[4]),
lwd = 3, col = "purple4")
abline (a = mean(coef(lm.4.sim)[2]), b = mean(coef(lm.4.sim)[2]),
lwd = 3, col = "purple4")
###
abline (a = mean(coef(lm.4.sim)[4]),
b = mean(coef(lm.4.sim)[4]),
lwd = 3, col = "purple4")
#--------------Rlab in the textbook
library (ISLR)
names(Smarket )
install.packages("ISLR")
#--------------Rlab in the textbook
library (ISLR)
names(Smarket )
[1] "Year"     "Lag1"     "Lag2"     "Lag3"     "Lag4"
[6] "Lag5"     "Volume"   "Today"    "Direction"
names(Smarket )
"Year"     "Lag1"     "Lag2"     "Lag3"     "Lag4"
"Lag5"     "Volume"   "Today"    "Direction"
dim(Smarket)
names(Smarket )
summary(Smarket)
cor(Smarket)
cor(Smarket[,-9])
Smarket[,-9]
cor(Smarket[,-9]) #in cor, x must be numeric
#dframe[rowsyouwant, columnsyouwant]
#Is using [,-9] to get all rows and columns?
attach(Smarket)
plot(Volume)
#------Logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket, family=binomial) #same as linear
#but have to add argument: family=binomial
Call:
flm(formula = Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket, family=binomial)
#but have to add argument: family=binomial
Call:
glm(formula = Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket, family=binomial)
glm(formula = Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket, family=binomial)
#------Logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket, family=binomial) #same as linear
glm(formula = Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket, family=binomial)
#but have to add argument: family=binomial
summary(flm.fit)
#but have to add argument: family=binomial
summary(glm.fit)
#predict() function to predict the prob that the market will go up
##given values of the predictors.
##type="response" tells R to output prob of the form P(Y=1|X) as opposed to logit.
glm.probs=predict(glm.fit,type="response")
glm.probs[1:10]
contrasts(Direction)
#Convert probs into class labels Up or Down
glm.pred=rep("Down",1250)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction) #confusion market to see how many correct/incorrectly classified
mean(glm.pred==Direction)
#Predict right 52% of the time with logistic regression
Smarket.2005=Smarket[!train,]
train=(Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
irection.2005=Direction[!train]
Direction.2005=Direction[!train]
flm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial,subset=train)
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial,subset=train)
glm.probs=predict(glm.fit,Smarket.2005,type="response")
#Training is b4 2005, testing only dates in 2005
#Finally, compute predictions for 2005 and compare them to the actual movements of the market
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
mean(glm.pred!=Direction.2005)
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
106/(106+76)
#Predict returns associated w Lag1 & Lag2
predict(glm.fit,newdata=data.frame(Lag1=c(1.2,1.5),
Lag2=c(1.1,-0.8)),
type="response")
#Predict returns associated w Lag1 & Lag2
predict(glm.fit,newdata=data.frame(Lag1=c(1.2,1.5)),
Lag2=c(1.1,-0.8)),
type="response")
#Predict returns associated w Lag1 & Lag2
predict(glm.fit,newdata=data.frame(Lag1=c(1.2,1.5),
Lag2=c(1.1,-0.8)),
type="response")
#Predict returns associated w Lag1 & Lag2
predict(glm.fit,newdata=data.frame(Lag1=c(1.2,1.5),
Lag2=c(1.1,-0.8)),
type="response")
library(arm)
library(Matching)
data("lalonde")
names(lalonde)
glm.fit(treat ~ educ+age+married+married,data=lalonde,family=binomial)
lalo.fit = glm.fit(treat ~ educ+age+married+married,data=lalonde,family=binomial)
lalo.fit = glm.fit(treat ~ educ+age+married+married,data=lalonde,family=binomial)
lalo.fit = glm.fit(treat~educ+age+married,data=lalonde,family=binomial)
lalo.fit=glm.fit(treat~educ+age+married,data=lalonde,family=binomial)
library(arm)
library(Matching)
data("lalonde")
names(lalonde)
lalo.fit=glm.fit(treat~educ+age+married,data=lalonde,family=binomial)
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket, family=binomial) #same as linear
#but have to add argument: family=binomial
summary(glm.fit) #smallest p-value is with Lag1
#However, at a value of 0.15, the p-value is still large
#-> no clear evidence of real association Lag1 & Direction
glm(formula = Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket, family=binomial)
lalo.fit=glm.fit(treat~educ+age+married,data=lalonde,family=binomial)
lalo.fit=glm.fit(treat~educ+age+married,family=binomial)
library(arm)
library(Matching)
data("lalonde")
names(lalonde)
lalo.fit=glm.fit(treat~educ+age+married,data=lalonde,family=binomial)
lalo.fit=glm.fit(treat~educ+age+married+hisp+black,data=lalonde,family=binomial)
library(arm)
library(Matching)
data(lalonde)
names(lalonde)
lalo.fit=glm.fit(treat~educ+age+married+hisp+black,data=lalonde,family=binomial)
lalofit=glm.fit(treat~educ+age+married+hisp+black,data=lalonde,family=binomial)
summary(lalo.fit)
lalo.fit=glm(treat~educ+age+married+hisp+black,data=lalonde,family=binomial)
summary(lalo.fit)
lalo.glm(formula=treat~educ+age+married+hisp+black,data=lalonde,family=binomial)
glm(formula=treat~educ+age+married+hisp+black,data=lalonde,family=binomial)
lalo.fit=glm(treat~educ+age+married+hisp+black,data=lalonde,family=binomial)
summary(lalo.fit)
glm(formula=treat~educ+age+married+hisp+black,data=lalonde,family=binomial)
pros.glm(predict.glm(lalo.fit, type="response"))
pros.glm = predict.glm(lalo.fit, type="response")
pros.glm = predict.glm(lalo.fit, type="response")
contrasts(treat)
pros.pred=rep("No",1250)
pros.pred[glm.probs>.5]
table(pros.pred, treat)
mean(pros.pred==treat)
glm.probs[1:10]
lalonde$treat
pros.glm = predict.glm(lalo.fit, type="response")
contrasts(treat)
pros.pred=rep("No",1250)
pros.glm = predict.glm(lalo.fit, type="response")
contrasts(treat)
pros.glm = predict.glm(lalo.fit, type="response")
pros.pred[1:10]
contrasts(treat)
pros.pred=rep("No",1250)
pros.pred[glm.probs>.5]
table(pros.pred, treat)
contrasts(lalonde$treat)
Smarket$Direction
pros.glm = predict.glm(lalo.fit, type="response")
pros.pred[1:10]
contrasts(treat)
pros.pred=rep("1",1250)
pros.pred[glm.probs>.5]
table(pros.pred, treat)
mean(pros.pred==treat)
pros.glm = predict(lalo.fit,type="response")
pros.pred[1:10]
contrasts(treat)
pros.pred=rep("1",1250)
pros.pred[pros.glm>.5]
table(pros.pred, treat)
mean(pros.pred==treat)
#predict() function to predict the prob that the market will go up
##given values of the predictors.
##type="response" tells R to output prob of the form P(Y=1|X) as opposed to logit.
glm.probs=predict(glm.fit,type="response")
glm.probs[1:10]
pros.glm = predict(lalo.fit,type="response")
pros.pred[1:10]
contrasts(treat)
lalonde$treat
pros.glm = predict(lalo.fit,type="response")
pros.pred[1:10]
pros.pred=rep("1",1250)
pros.pred[pros.glm>.5]
table(pros.pred, treat)
library(arm)
library(Matching)
data(lalonde)
names(lalonde)
lalo.fit=glm(treat~educ+age+married+hisp+black,data=lalonde,family=binomial)
summary(lalo.fit) #see summary w pvalue
glm(formula=treat~educ+age+married+hisp+black,data=lalonde,family=binomial) #see intercept, shortened version
pros.glm = predict(lalo.fit,type="response")
pros.pred[1:10]
library(arm)
library(Matching)
data(lalonde)
names(lalonde)
lalo.fit=glm(treat~educ+age+married+hisp+black,data=lalonde,family=binomial)
summary(lalo.fit) #see summary w pvalue
glm(formula=treat~educ+age+married+hisp+black,data=lalonde,family=binomial) #see intercept, shortened version
pros.glm = predict(lalo.fit,type="response")
pros.pred[1:10]
pros.pred=rep("1",1250)
pros.pred[pros.glm>.5]
table(pros.pred, lalonde$treat)
mean(pros.pred==lalonde$treat)
dim(lalonde)
pros.pred=rep("1",445)
pros.pred[pros.glm>.5]
table(pros.pred, lalonde$treat)
mean(pros.pred==lalonde$treat)
dim(Smarket)
contrasts(treat)
pros.pred=rep("1",445) #Use 445 because dim(lalonde) is 445,12
pros.pred[pros.glm>.5]
table(pros.pred, lalonde$treat)
mean(pros.pred==lalonde$treat)
dim(Smarket)
#---------Use all of data as partly test and partly training data
#Now create test data
lalonde
Smarket
#---------Use all of data as partly test and partly training data
#Now create test data
lalonde
#---------Use all of data as partly test and partly training data
#Now create test data
train= lalonde$treat[1:222]
lalonde.sub = lalonde[!train,]
dim(lalonde.sub)
lalo.fit=glm(treat~educ+age+married+hisp+black,data=lalonde,family=binomial,subset=train)
lalo=glm(treat~educ+age+married+hisp+black,data=lalonde,family=binomial,subset=train)
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data=Smarket,family=binomial,subset=train)
treat.sub = treat[!train,]
treat.sub = lalonde$treat[!train,]
#---------Use all of data as partly test and partly training data
#Now create test data
train= lalonde$treat[1:222]
treat.sub = lalonde$treat[!train,]
treat.sub=treat[!train,]
#---------Use all of data as partly test and partly training data
#Now create test data
train= lalonde$treat[1:222]
lalonde.sub = lalonde[!train,]
treat.sub=treat[!train,]
lalo.fit=glm(treat~educ+age+married+hisp+black,data=lalonde,family=binomial,subset=train)
pros.proh=predict(lalo.fit,lalonde.sub,type="response")
lalonde$educ
library(ISLR)
set.seed(1)
train = sample(392,196)
set.seed(20181001)
# Load packages and data
library(Matching)
library(boot)
data(lalonde)
# Train your model on ALL the data -- Use glm instead of lm
glm.lalonde <- glm(re78 ~ ., data = lalonde)
glm2.lalonde <- glm(re78 ~ .*., data = lalonde)  # all predictors with all first-order interaction terms
# cv.glm performs LOOCV (You can use the K argument to perform K-fold CV)
cv.err <- cv.glm(lalonde, glm.lalonde)
cv.err2 <- cv.glm(lalonde, glm2.lalonde)
# This is the MSE
cv.err$delta[1]
cv.err2$delta[1]  # The second model overfits the data and has a larger MSE
# Declare the auxiliary function
boot.fn <- function(data, index) return(coef(lm(re78 ~ ., data = data, subset = index)))
boot.fn(lalonde, 1:nrow(lalonde)) # Check whether the function is working as expected
boot.lalonde <- boot(lalonde, boot.fn, 10000)
# Retrieve standard errors
boot.se <- apply(boot.lalonde$t, 2, sd)
lm.se <- summary(lm(re78 ~ ., data = lalonde))$coef[,2]
format(data.frame(boot.se, lm.se), digits=2, scientific=F)
conf.intervals <- apply(boot.lalonde$t, 2, quantile, c(0.025, 0.975))
set.seed(20181001)
# Load packages and data
library(Matching)
library(boot)
data(lalonde)
# Train your model on ALL the data -- Use glm instead of lm
glm.lalonde <- glm(re78 ~ ., data = lalonde)
glm2.lalonde <- glm(re78 ~ .*., data = lalonde)  # all predictors with all first-order interaction terms
# cv.glm performs LOOCV (You can use the K argument to perform K-fold CV)
cv.err <- cv.glm(lalonde, glm.lalonde)
cv.err2 <- cv.glm(lalonde, glm2.lalonde)
# This is the MSE
cv.err$delta[1]
cv.err2$delta[1]  # The second model overfits the data and has a larger MSE
# Declare the auxiliary function
boot.fn <- function(data, index) return(coef(lm(re78 ~ ., data = data, subset = index)))
boot.fn(lalonde, 1:nrow(lalonde)) # Check whether the function is working as expected
boot.lalonde <- boot(lalonde, boot.fn, 10000)
# Retrieve standard errors
boot.se <- apply(boot.lalonde$t, 2, sd)
lm.se <- summary(lm(re78 ~ ., data = lalonde))$coef[,2]
format(data.frame(boot.se, lm.se), digits=2, scientific=F)
conf.intervals <- apply(boot.lalonde$t, 2, quantile, c(0.025, 0.975))
# Retrieve standard errors
boot.se <- apply(boot.lalonde$t, 2, sd)
lm.se <- summary(lm(re78 ~ ., data = lalonde))$coef[,2]
format(data.frame(boot.se, lm.se), digits=2, scientific=F)
conf.intervals <- apply(boot.lalonde$t, 2, quantile, c(0.025, 0.975))
conf.intervals
quantile(conf.intervals)
quantile(boot.se)
sigma?
?sigma
